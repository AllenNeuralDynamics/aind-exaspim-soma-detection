"""
Created on Fri Jan 3 12:30:00 2025

@author: Anna Grim
@email: anna.grim@alleninstitute.org

Code used to train neural network to classify somas proposals.

"""

from sklearn.metrics import (
    accuracy_score,
    f1_score,
    precision_score,
    recall_score,
)

import numpy as np
import os
import torch

from aind_exaspim_soma_detection import soma_proposal_classification as spc


def validate_model(writer, dataloader, model, model_dir, device, cnt, best_f1):
    _, hat_y, y = spc.run_inference(dataloader, model, device, False)
    f1 = evaluation_metrics(writer, hat_y, y, cnt, "validation", 0.5)
    if f1 > best_f1:
        best_f1 = f1
        model_name = f"model_{cnt}_f1={round(f1, 4)}.pth"
        model_path = os.path.join(model_dir, model_name)
        torch.save(model.state_dict(), model_path)
        print("New Best:", model_path)
    else:
        print("F1 score:", f1)
    return best_f1


def evaluation_metrics(writer, hat_y, y, cnt, prefix="", threshold=0):
    """
    Computes and logs various evaluation metrics to a TensorBoard.

    Parameters
    ----------
    writer : torch.utils.tensorboard.SummaryWriter
        TensorBoard writer object to log the metrics.
    hat_y : numpy.ndarray
        Predicted generated by a neural network.
    y : numpy.ndarray
        True labels.
    cnt : int
        Number of gradient updates. Used as the x-axis value for logging
        metrics in the TensorBoard.
    prefix : str, optional
        String prefix to prepend to the metric names when logging to
        TensorBoard. The default is an empty string.
    threshold : float, optional
        Value that is used to threshold "hat_y" to obtain positive and
        negative predictions. The default is 0.

    Returns
    -------
    float
        F1 score for the given epoch.

    """
    # Compute metrics
    hat_y = (hat_y > threshold).astype(int)
    accuracy = accuracy_score(y, hat_y)
    accuracy_dif = accuracy - np.sum(y) / len(y)
    f1 = f1_score(y, hat_y)
    precision = precision_score(y, hat_y)
    recall = recall_score(y, hat_y)

    # Write results to tensorboard
    writer.add_scalar(prefix + "_accuracy", accuracy, cnt)
    writer.add_scalar(prefix + "_accuracy_df", accuracy_dif, cnt)
    writer.add_scalar(prefix + "_precision:", precision, cnt)
    writer.add_scalar(prefix + "_recall:", recall, cnt)
    writer.add_scalar(prefix + "_f1:", f1, cnt)
    return f1
