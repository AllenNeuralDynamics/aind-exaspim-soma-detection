"""
Created on Fri Jan 3 12:30:00 2025

@author: Anna Grim
@email: anna.grim@alleninstitute.org

Code used to train neural network to classify somas proposals.

"""

from sklearn.metrics import (
    accuracy_score,
    f1_score,
    precision_score,
    recall_score,
)
import torch.nn.functional as F

import numpy as np
import os
import torch
import torch.nn as nn

from aind_exaspim_soma_detection import soma_proposal_classification as spc


# --- Loss Functions ---
class FocalLoss(nn.Module):
    """
    Focal Loss for addressing class imbalance in binary classification tasks.

    """

    def __init__(self, alpha=0.5, gamma=2.0):
        """
        Initializes a Focal Loss module for a given alpha and gamma.

        Parameters
        ----------
        alpha : float, optional
            Factor that controls the balance between positive and negative
            examples. The default is 0.5.
        gamma : float, optional
            Focusing parameter that controls the rate at which easy examples
            are down-weighted. The default is 2.0.

        """
        super(FocalLoss, self).__init__()
        self.alpha = alpha
        self.gamma = gamma

    def forward(self, hat_y, y):
        """
        Computes the focal loss between the predicted values and the ground
        truth labels.

        Parameters
        ----------
        hat_y : torch.Tensor
            Predictions generated by neural network.
        y : torch.Tensor
            Ground truth labels.

        Returns
        -------
        torch.Tensor
            Focal loss value.

        """
        BCE_loss = F.binary_cross_entropy(hat_y, y, reduction="none")
        pt = torch.exp(-BCE_loss)
        return torch.mean(self.alpha * (1 - pt) ** self.gamma * BCE_loss)


# --- Validation ---
def validate_model(writer, dataloader, model, model_dir, device, cnt, best_f1):
    _, hat_y, y = spc.run_inference(dataloader, model, device, False)
    f1 = evaluation_metrics(writer, hat_y, y, cnt, "validation", 0.5)
    if f1 > best_f1:
        best_f1 = f1
        model_name = f"model_{cnt}_f1={round(f1, 4)}.pth"
        model_path = os.path.join(model_dir, model_name)
        torch.save(model.state_dict(), model_path)
        print("New Best:", model_path)
    return best_f1


def evaluation_metrics(writer, hat_y, y, cnt, prefix="", threshold=0):
    """
    Computes and logs various evaluation metrics to a TensorBoard.

    Parameters
    ----------
    writer : torch.utils.tensorboard.SummaryWriter
        TensorBoard writer object to log the metrics.
    hat_y : numpy.ndarray
        Predictions generated by a neural network.
    y : numpy.ndarray
        Ground truth labels.
    cnt : int
        Number of gradient updates. Used as the x-axis value for logging
        metrics in the TensorBoard.
    prefix : str, optional
        String prefix to prepend to the metric names when logging to
        TensorBoard. The default is an empty string.
    threshold : float, optional
        Value that is used to threshold "hat_y" to obtain positive and
        negative predictions. The default is 0.

    Returns
    -------
    float
        F1 score for the given epoch.

    """
    # Compute metrics
    hat_y = (hat_y > threshold).astype(int)
    accuracy = accuracy_score(y, hat_y)
    accuracy_dif = accuracy - np.sum(y) / len(y)
    f1 = f1_score(y, hat_y)
    precision = precision_score(y, hat_y)
    recall = recall_score(y, hat_y)

    # Write results to tensorboard
    writer.add_scalar(prefix + "_accuracy", accuracy, cnt)
    writer.add_scalar(prefix + "_accuracy_df", accuracy_dif, cnt)
    writer.add_scalar(prefix + "_precision:", precision, cnt)
    writer.add_scalar(prefix + "_recall:", recall, cnt)
    writer.add_scalar(prefix + "_f1:", f1, cnt)
    return f1
